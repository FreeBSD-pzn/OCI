23.02.2022
Из запущенных 3-х виртуальных машин в OCI только пока одна была задействована для
запуска приложений в контейнерах Docker.

Оставшиеся две виртуальные машины могут быть использованы для запуска элементов
Kubernetes (k8s).

Первоначальный этап это запуск кластера Kubernetes на локальной базе.
Для данной цели будет использоваться macmini с установленным Virtualbox и 
запущенными в VirtualBox виртуальными машинами с ОС Ubuntu:
1. VM с Ubuntu 21.10 kernel 5.13.0-35-generic ОЗУ 4Мб CPU 2 core для master узла;
2. VM c Ubuntu 18.04.6 LTS kernel 4.15.0-171-generic ОЗУ 1 Мб CPU 1 core для node;
3. VM с Ubuntu 20.04.4 LTS kernel 5.4.0-104-generic ОЗУ 1 Мб CPU 1 core для node.

Описаний для установки и конфигурирования кластера Kubernetes в Интернет много,
вот к примеру https://infoit.com.ua/linux/kak-ustanovit-kubernetes-na-ubuntu-20-04-lts/

Единственное отличие заключается только в том, что мастер узел устанавливался на 
Ubuntu 21.10, и наименование серверов использовалось свое.

После установки необходимых пакетов на мастер узле и запуска:
$ sudo kubeadm init

Мастер узел установился и запустился без проблем, что называется "из коробки"
и по завершении было выведено следующее сообщение:
------------------------------------------------
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

sudo kubeadm join 192.168.2.90:6443 --token fzs4mj.xxxxxxxxxxxxxxxx (*) --discovery-token-ca-cert-hash
sha256:c7faaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (*)
------------------------------------------------
* опции --token и sha256: намеренно были изменены
------------------------------------------------

Попытка запустить рабочие (ведомые) узлы "из коробки" не увенчалась успехом. На рабочих (ведомых) узлах
целенаправленно были установлены ОС Ubuntu 18.04.6 и 20.04.4 для получения дополнительных навыков установки
и конфигурирования кластера Kubernetes.

На обоих узлах инициализация завершалась с одной и той же ошибкой. В интернете описано много способов как
устранить возникающую при инициализации ошибку, например:
https://www.devopszones.com/2019/03/kubelet-failed-to-run-kubelet-failed-to.html

После исправления docker.service на обоих ведомых узлах оба были инициализированы без ошибок и
получился кластер kubernetes состоящий из 3-х узлов.

===================
01.03.2022

Для устранения ошибок присоединения ведомых узлов в кластере Kubernetes необходимо внести изменения в
файлы docker.service в операционных системах Ubuntu 18.04.6 и 20.04.4 эти файлы расположены в разных местах.

для ubuntu 18.04.6 нужно редактировать следующий файл:
sudo vi /lib/systemd/system/docker.service

после редактирования раздел Service выглядит следующим образом:
[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

для ubuntu 20.04.4 нужно редактировать следующий файл:
sudo vi /usr/lib/systemd/system/docker.service

после редактирования раздел Service выглядит следующим образом:
[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

Как описано выше после редактирования и перезапуска сервисов ведомые узлы были инициализированы без ошибок и
получился кластер kubernetes состоящий из 3-х узлов.

===================
05.03.2022

Кластер Kubernetes запущен, пора установить kubectl на FreeBSD,

$ uname -a <Enter>
FreeBSD sony 11.3-RELEASE FreeBSD 11.3-RELEASE #0 r349754: Fri Jul  5 04:31:33 UTC 2019
root@releng2.nyi.freebsd.org:/usr/obj/usr/src/sys/GENERIC  i386

kubectl используется из ports [/usr/ports/sysutils/kubectl]

# cd /usr/ports/sysutils/kubectl
# make install
# make clean

Компиляция и установка прошла без проблем и что называется "из коробки"
Перед запуском kubectl необходимо скопировать файлы конфигурации в каталог
по умолчанию пользователя, под которым вы планируете работать, только копировать
необходимо с административного узла, скажем используя scp.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"", GitTreeState:"", BuildDate:"2022-03-08T23:28:04Z", GoVersion:"go1.12.7", Compiler:"gc", Platform:"freebsd/386"}
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.5", GitCommit:"c285e781331a3785a7f436042c65c5641ce8a9e9", GitTreeState:"clean", BuildDate:"2022-03-16T15:52:18Z", GoVersion:"go1.17.8", Compiler:"gc", Platform:"linux/amd64"}

Теперь вы можете использовать desktop с FreeBSD для управления кластером Kubernetes.

===================
09.03.2022

pznoleg.sony[~/VM/OCI] <1006-7>$ kubectl get no
NAME    STATUS   ROLES                  AGE   VERSION
ubu18   NotReady <none>                 6d    v1.23.4
ubu20   NotReady <none>                 6d    v1.23.4
ubunt   Ready    control-plane,master   6d    v1.23.4

Как можно видеть кластер из 3-х узлов запущен, но, видно что ведомые узлы
"не готовы". Для подготовки кластера Kubernetes в рабочее состояние необходимо
установить расширения для обеспечения работы "сети кластера Kubernetes".

Например в следующем примере
https://infoit.com.ua/linux/kak-ustanovit-kubernetes-na-ubuntu-20-04-lts/
предлагается установить Flannel, но в моем случае установка "из коробки"
не получилась, так как данное расширение работало со сбоями - на административном
узле постоянно завершался с ошибкой один из контейнеров и через сутки по команде
docker ps -a выдавался большой список завершившихся контейнеров, поэтому мне
пришлось удалить данный пакет и мной был взят другой пакет: Weave Net.
Weave Net установился что называется "из коробки".

По следующей ссылке можно посмотреть список пакетов для обеспечения работы сети:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

Теперь кластер должен быть готов, проверим:
pznoleg.sony[~/VM/OCI] <1009-10>$ kubectl get no
NAME    STATUS   ROLES                  AGE   VERSION
ubu18   Ready    <none>                 6d    v1.23.4
ubu20   Ready    <none>                 6d    v1.23.4
ubunt   Ready    control-plane,master   6d    v1.23.4

===================
16.03.2022
Небольшое замечание:
Расширение Flannel удалить "чисто" не получилось и кластер Kubernetes пришлось
инициализировать с нуля с последующей установкой Weave Net.

Как можно видеть установленный kubectl на FreeBSD следующей версии:
version.Info {Major:"1", Minor:"15", GitVersion:"v1.15.0"}

На Ubuntu была установлена следующая версия kubectl:
oleg.Ubunt[~] <998-1>$ kubectl version
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.4", GitCommit:"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a", GitTreeState:"clean", BuildDate:"2022-02-16T12:38:05Z", GoVersion:"go1.17.7", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.5", GitCommit:"c285e781331a3785a7f436042c65c5641ce8a9e9", GitTreeState:"clean", BuildDate:"2022-03-16T15:52:18Z", GoVersion:"go1.17.8", Compiler:"gc", Platform:"linux/amd64"}

Разные версии kubectl поддерживают разные опции командной строки.

!!! Так, к примеру, в kubectl run в Ubuntu отсутствует опция --generator,
в то время как в FreeBSD эта опция присутствует и можно ввести следующую команду:
kubectl run lesson02 --image=goodbyespy/lesson02 --port=8080 --generator=run-pod/v1

Для экспериментов можно использовать данные в каталоге k8s/Lesson02.
Образ Docker можно найти в hub.docker.com под именем goodbyespy/lesson02.

Можете поэксперементировать, к примеру попробуйте следующую команду:
kubectl api-resources | awk '{ print $1}' | sort | xargs -L 1 kubectl get
